https://www.youtube.com/watch?v=xNY9cbaLuGk

https://github.com/NVIDIA/mig-parted
# Install Go ... 

nvidia-mig-parted export
nvidia-smi -L
dpkg -l | grep nvidia-container; dpkg -l | grep nvidia-docker 
cat /etc/docker/daemon.json
k get nodes -o json | jq .items[0].status.allocatable

helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo add nvgfd https://nvidia.github.io/gpu-feature-discovery
helm repo update

helm install \
  --version=0.9.0\
  --set migStrategy=mixed \
  nvidia-device-plugin \
  nvdp/nvidia-device-plugin

helm install \
  --version=0.4.1\
  --set migStrategy=mixed \
  gpu-feature-discovery \
  nvgfd/gpu-feature-discovery

helm list
k get nodes -o json | jq '.items[0].status.allocatable | with_entries(select(.key | contains("nvidia")))'
k get nodes -o json | jq '.items[0].metadata.labels | with_entries(select(.key | contains("nvidia")))'

kubectl run -it --rm \
  --image=nvidia/cuda:11.0-base \
  --restart=Never \
  --limits=nvidia.com/gpu=1 \
  gpu-pod -- nvidia-smi -L

cat /etc/nvidia-mig-manager/config.yaml

nvidia-mig-parted export

time nvidia-mig-parted apply -c all-1g.5gb
./nvidia-mig-parted apply -f examples/config.yaml -c all-1g.5gb
./nvidia-mig-parted assert --mode-only -f examples/config.yaml -c all-1g.5gb

kubectl run -it --rm \
  --image=nvidia/cuda:11.0-base \
  --restart=Never \
  --limits=nvidia.com/gpu=mig-1g.5gb=1 \
  mig-pod -- bash -x -c "
      nvidia-smi -L;           echo"";
      ls -la /dev/nvidia*;     echo"";
      find /proc/driver/nvidia/capabilities;
  "

# Go single mode
helm install \
  --version=0.9.0\
  --set migStrategy=single \
  nvidia-device-plugin \
  nvdp/nvidia-device-plugin

helm install \
  --version=0.4.1\
  --set migStrategy=single \
  gpu-feature-discovery \
  nvgfd/gpu-feature-discovery

k get nodes -o json | jq '.items[0].status.allocatable | with_entries(select(.key | contains("nvidia")))'
k get nodes -o json | jq '.items[0].metadata.labels | with_entries(select(.key | contains("nvidia")))'


kubectl run -it --rm \
  --image=nvidia/cuda:11.0-base \
  --restart=Never \
  --limits=nvidia.com/gpu=1 \
  gpu-pod -- bash -x -c "
      nvidia-smi -L;           echo"";
      ls -la /dev/nvidia*;     echo"";
      find /proc/driver/nvidia/capabilities;
  "
  
  
  











####################################################################
[root@worker-gpu-7 ~]# nvidia-smi mig -lgip
No MIG-enabled devices found.

# Enable MIG
[root@worker-gpu-7 ~]# nvidia-smi -mig 1
Enabled MIG Mode for GPU 00000000:61:00.0

Warning: persistence mode is disabled on device 00000000:61:00.0. See the Known Issues section of the nvidia-smi(1) man page for more information. Run with [--help | -h] switch to get more information on how to enable persistence mode.
All done.




# Reset
[root@worker-gpu-7 ~]# nvidia-smi -r
GPU 00000000:61:00.0 was successfully reset.
All done.

# List profiles ID
[root@worker-gpu-7 ~]# nvidia-smi mig -i 0 -lgip
+--------------------------------------------------------------------------+
| GPU instance profiles:                                                   |
| GPU   Name          ID    Instances   Memory     P2P    SM    DEC   ENC  |
|                           Free/Total   GiB              CE    JPEG  OFA  |
|==========================================================================|
|   0  MIG 1g.5gb     19     7/7        4.75       No     14     0     0   |
|                                                          1     0     0   |
+--------------------------------------------------------------------------+
|   0  MIG 2g.10gb    14     3/3        9.75       No     28     1     0   |
|                                                          2     0     0   |
+--------------------------------------------------------------------------+
|   0  MIG 3g.20gb     9     2/2        19.62      No     42     2     0   |
|                                                          3     0     0   |
+--------------------------------------------------------------------------+
|   0  MIG 4g.20gb     5     1/1        19.62      No     56     2     0   |
|                                                          4     0     0   |
+--------------------------------------------------------------------------+
|   0  MIG 7g.40gb     0     1/1        39.50      No     98     5     0   |
|                                                          7     1     1   |
+--------------------------------------------------------------------------+


nvidia-smi mig  -cgi 19,19,19,19,19,19,19 -C

nvidia-smi mig  -cgi 14,14,14 -C


[root@worker-gpu-7 ~]# for i in {1..7}; do nvidia-smi mig -i 0 -cgi 19; done

Successfully created GPU instance ID 13 on GPU  0 using profile MIG 1g.5gb (ID 19)
Successfully created GPU instance ID 11 on GPU  0 using profile MIG 1g.5gb (ID 19)
Successfully created GPU instance ID 12 on GPU  0 using profile MIG 1g.5gb (ID 19)
Successfully created GPU instance ID  7 on GPU  0 using profile MIG 1g.5gb (ID 19)
Successfully created GPU instance ID  8 on GPU  0 using profile MIG 1g.5gb (ID 19)
Successfully created GPU instance ID  9 on GPU  0 using profile MIG 1g.5gb (ID 19)
Successfully created GPU instance ID 10 on GPU  0 using profile MIG 1g.5gb (ID 19)


#    [-dgi | --destroy-gpu-instance]: Destroy GPU instances.
[root@worker-gpu-7 ~]# for i in {1..7}; do nvidia-smi mig -i 0 -dgi ; done


[root@worker-gpu-7 ~]# nvidia-smi mig -i 0 -lcip
+--------------------------------------------------------------------------------------+
| Compute instance profiles:                                                           |
| GPU     GPU       Name             Profile  Instances   Exclusive       Shared       |
|       Instance                       ID     Free/Total     SM       DEC   ENC   OFA  |
|         ID                                                          CE    JPEG       |
|======================================================================================|
|   0      7       MIG 1g.5gb           0*     1/1           14        0     0     0   |
|                                                                      1     0         |
+--------------------------------------------------------------------------------------+
|   0      8       MIG 1g.5gb           0*     1/1           14        0     0     0   |
|                                                                      1     0         |
+--------------------------------------------------------------------------------------+
|   0      9       MIG 1g.5gb           0*     1/1           14        0     0     0   |
|                                                                      1     0         |
+--------------------------------------------------------------------------------------+
|   0     10       MIG 1g.5gb           0*     1/1           14        0     0     0   |
|                                                                      1     0         |
+--------------------------------------------------------------------------------------+
|   0     11       MIG 1g.5gb           0*     1/1           14        0     0     0   |
|                                                                      1     0         |
+--------------------------------------------------------------------------------------+
|   0     12       MIG 1g.5gb           0*     1/1           14        0     0     0   |
|                                                                      1     0         |
+--------------------------------------------------------------------------------------+
|   0     13       MIG 1g.5gb           0*     1/1           14        0     0     0   |
|                                                                      1     0         |
+--------------------------------------------------------------------------------------+


# for each GPU Instance ID and profile ID
[root@worker-gpu-7 ~]# for i in {7..13}; do nvidia-smi mig -gi $i -cci 0\* ; done

#    [-dci | --destroy-compute-instance]: Destroy compute instances.
[root@worker-gpu-7 ~]# for i in {7..13}; do nvidia-smi mig -dci -ci 0 -gi $i  ; done
Unable to destroy compute instance ID  0 from GPU  0 GPU instance ID  7: In use by another client
Failed to destroy compute instances: In use by another client
Successfully destroyed compute instance ID  0 from GPU  0 GPU instance ID  8
Successfully destroyed compute instance ID  0 from GPU  0 GPU instance ID  9
Successfully destroyed compute instance ID  0 from GPU  0 GPU instance ID 10
Successfully destroyed compute instance ID  0 from GPU  0 GPU instance ID 11
Successfully destroyed compute instance ID  0 from GPU  0 GPU instance ID 12
Successfully destroyed compute instance ID  0 from GPU  0 GPU instance ID 13


# et zou
[root@worker-gpu-7 ~]# nvidia-smi
Sat Mar  6 15:32:16 2021      
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-PCIE-40GB      Off  | 00000000:61:00.0 Off |                   On |
| N/A   75C    P0    51W / 250W |     13MiB / 40536MiB |     N/A      Default |
|                               |                      |              Enabled |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| MIG devices:                                                                |
+------------------+----------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |
|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|
|                  |                      |        ECC|                       |
|==================+======================+===========+=======================|
|  0    7   0   0  |      1MiB /  4864MiB | 14      0 |  1   0    0    0    0 |
|                  |      0MiB /  8191MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
|  0    8   0   1  |      1MiB /  4864MiB | 14      0 |  1   0    0    0    0 |
|                  |      0MiB /  8191MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
|  0    9   0   2  |      1MiB /  4864MiB | 14      0 |  1   0    0    0    0 |
|                  |      0MiB /  8191MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
|  0   10   0   3  |      1MiB /  4864MiB | 14      0 |  1   0    0    0    0 |
|                  |      0MiB /  8191MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
|  0   11   0   4  |      1MiB /  4864MiB | 14      0 |  1   0    0    0    0 |
|                  |      0MiB /  8191MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
|  0   12   0   5  |      1MiB /  4864MiB | 14      0 |  1   0    0    0    0 |
|                  |      0MiB /  8191MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
|  0   13   0   6  |      1MiB /  4864MiB | 14      0 |  1   0    0    0    0 |
|                  |      0MiB /  8191MiB |           |                       |
+------------------+----------------------+-----------+-----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+



# la doc
[root@worker-gpu-7 ~]# nvidia-smi mig

    mig -- Multi Instance GPU management.

    Usage: nvidia-smi mig [options]

    Options include:
    [-h | --help]: Display help information.
    [-i | --id]: Enumeration index, PCI bus ID or UUID.
                 Provide comma separated values for more than one device.
    [-gi | --gpu-instance-id]: GPU instance ID.
                               Provide comma separated values for more than one GPU instance.
    [-ci | --compute-instance-id]: Compute instance ID.
                                   Provide comma separated values for more than one compute
                                   instance.
    [-lgip | --list-gpu-instance-profiles]: List supported GPU instance profiles.
                                            Option -i can be used to restrict the command to
                                            run on a specific GPU.
    [-lgipp | --list-gpu-instance-possible-placements]: List possible GPU instance placements
                                                        in the following format, {Start}:Size.
                                                        Option -i can be used to restrict the
                                                        command to run on a specific GPU.
    [-C | --default-compute-instance]: Create compute instance with the default profile when used
                                       with the option to create a GPU instance (-cgi).
    [-cgi | --create-gpu-instance]: Create GPU instances for the given profile tuples. A profile
                                    tuple consists of a profile name or ID and an optional placement
                                    specifier, which consists of a colon and a placement start index.
                                    Provide comma separated values for more than one profile tuple.
                                    Option -i can be used to restrict the command to run on
                                    a specific GPU.
    [-dgi | --destroy-gpu-instance]: Destroy GPU instances.
                                     Options -i and -gi can be used individually or combined
                                     to restrict the command to run on a specific GPU or GPU
                                     instance.
    [-lgi | --list-gpu-instances]: List GPU instances.
                                   Option -i can be used to restrict the command to run on a
                                   specific GPU.
    [-lcip | --list-compute-instance-profiles]: List supported compute instance profiles.
                                                Options -i and -gi can be used individually or
                                                combined to restrict the command to run on a
                                                specific GPU or GPU instance.
    [-cci | --create-compute-instance]: Create compute instance for the given profile name or IDs.
                                        Provide comma separated values for more than one profile.
                                        If no profile name or ID is given, then the default*
                                        compute instance profile ID will be used. Options -i and
                                        -gi can be used individually or combined to restrict the
                                        command to run on a specific GPU or GPU instance.
    [-dci | --destroy-compute-instance]: Destroy compute instances.
                                         Options -i, -gi and -ci can be used individually or
                                         combined to restrict the command to run on a specific
                                         GPU or GPU instance or compute instance.
    [-lci | --list-compute-instances]: List compute instances.
                                       Options -i and -gi can be used individually or combined
                                       to restrict the command to run on a specific GPU or GPU
                                       instance.
